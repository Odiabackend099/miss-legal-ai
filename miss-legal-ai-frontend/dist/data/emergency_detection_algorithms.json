{"extracted_information": "The provided web content is an academic paper titled \"Deep Learning-Based Portable Device for Audio Distress Signal Recognition in Urban Areas\". It describes the design, development, and evaluation of a portable, low-cost device for detecting audio distress signals (shouts) in noisy urban environments using deep learning models. The paper covers the creation of a new dataset, the feature extraction methods used, the deep learning architectures explored, the experimental results comparing different models and against existing work, and the real-time implementation on a portable device.\n\nThe core of the system is a deep multi-headed 2D convolutional neural network (CNN) that processes temporal and frequency features extracted from audio signals. The system was trained and validated using a custom-collected database of distress shouts and background noise recorded in real urban scenarios with a low-cost microphone. The trained model was successfully deployed on a Raspberry Pi for real-time operation.\n\nKey aspects covered include the hardware components (Raspberry Pi, microphone), the process of collecting a real-world dataset, the use of Mel spectrogram and Mel frequency cepstral coefficients (MFCCs) as features, the comparison of 1D CNN, 2D CNN, and multi-headed 2D CNN architectures, performance evaluation using recognition rates and false positive rates, and the computational performance metrics on the deployed device.", "specifications": {"device_hardware": {"main_processor": "Raspberry Pi 4 model B", "processor_specs": "64-bit quad-core processor", "ram": "up to 4 GB", "microphone": "Snowball iCE Condenser microphone", "microphone_type": "Pressure Gradient with USB digital output", "microphone_polar_pattern": "cardioid", "microphone_frequency_response": "even for human voice range"}, "audio_data": {"sampling_rate": "22,050 Hz", "window_size": "450 ms", "window_overlap": "112 ms"}, "dataset": {"name": "Custom-collected database", "recording_environment": "Real urban scenarios", "noise_levels": "low, medium, and high background noise", "microphone_type_used_for_collection": "low-cost", "distress_signals_collected": ["Wordless shout 'Ahhhhh' (SN)", "'auxilio' (help) (HN)", "'ayuda' (aid) (AN)"], "other_class": "Background noise (BN)", "subjects_recorded": "18 male and 5 female university undergraduate students (18-25 years)", "total_audio_files": "24", "total_duration": "~48 minutes", "events_per_class_in_dataset": "648 positive events per class", "dataset_distribution": {"BN": "20,645 samples (82.04%)", "SN": "1228 samples (4.88%)", "AN": "1771 samples (7.04%)", "HN": "1521 samples (6.04%)"}, "availability": "Publicly available (GitHub link provided)"}}, "pricing": {"total_implementation_cost": "~ $150 USD", "microphone_cost": "$50 USD", "raspberry_pi_cost": "$60 USD", "additional_elements_cost": "Includes micro-sd card, case, and stand", "comparison_to_high_quality_microphone": "> $450 USD"}, "features": [{"name": "Mel Spectrogram", "description": "A 2-dimensional signal representing time and frequency content. Computed using short-term Fourier transform over overlapping windows, with frequencies scaled according to the Mel's scale. Used as input to 2D CNN and Multi-Headed 2D CNN."}, {"name": "Mel Frequency Cepstral Coefficients (MFCCs)", "description": "Discrete cosine transform of a Mel frequency spectrogram. Provides an alternative representation. Used as input to 2D CNN and Multi-Headed 2D CNN."}, {"name": "Raw Audio Signal", "description": "The unprocessed audio data. Used as input to the 1D CNN architecture."}], "statistics": {"performance_on_reference_dataset": {"reference_dataset_description": "Created in [8], sampled at 32 KHz, 16 bits resolution, recorded with high-quality microphone, artificial white additive Gaussian noise. Classes: screams (S), glass breaking (GB), gun shot (GS), background noise (BN). Audio resampled to 22,050 Hz for comparison.", "reference_method_performance (SVM)": {"average_recognition_rate": "86.7%", "false_positive_rate": "2.6%"}, "our_method_performance (Multi-headed 2D CNN)": {"average_recognition_rate": "88.16%", "false_positive_rate": "0.91%", "confusion_matrix_percentages": {"True Class GB": {"Predicted GB": "92.8%", "Predicted GS": "0.0%", "Predicted S": "0.0%", "Predicted BN": "7.2%"}, "True Class GS": {"Predicted GB": "0.33%", "Predicted GS": "85.67%", "Predicted S": "0.0%", "Predicted BN": "14.0%"}, "True Class S": {"Predicted GB": "0.0%", "Predicted GS": "0.0%", "Predicted S": "86.0%", "Predicted BN": "14.0%"}, "True Class BN": {"Predicted GB": "0.33%", "Predicted GS": "0.1%", "Predicted S": "0.48%", "Predicted BN": "99.09%"}}}}, "performance_on_custom_dataset": {"evaluation_method": "k-fold cross-validation (k=8), each fold contains audio from one person", "1D_CNN_performance (Raw data)": {"average_recognition_rate": "59.67%", "false_positive_rate": "5.6%", "confusion_matrix_percentages": {"True Class BN": {"Predicted BN": "94%", "Predicted SN": "1.4%", "Predicted HN": "1.6%", "Predicted AN": "2.6%"}, "True Class SN": {"Predicted BN": "9.8%", "Predicted SN": "63%", "Predicted HN": "5.5%", "Predicted AN": "21%"}, "True Class HN": {"Predicted BN": "12%", "Predicted SN": "5.9%", "Predicted HN": "63%", "Predicted AN": "19%"}, "True Class AN": {"Predicted BN": "13%", "Predicted SN": "16%", "Predicted HN": "18%", "Predicted AN": "53%"}}}, "2D_CNN_performance (MFCCs)": {"average_recognition_rate": "69.3%", "false_positive_rate": "5.6%", "confusion_matrix_percentages": {"True Class BN": {"Predicted BN": "94%", "Predicted SN": "1.5%", "Predicted HN": "2.5%", "Predicted AN": "1.6%"}, "True Class SN": {"Predicted BN": "8.8%", "Predicted SN": "66%", "Predicted HN": "12%", "Predicted AN": "12%"}, "True Class HN": {"Predicted BN": "7.3%", "Predicted SN": "3.5%", "Predicted HN": "81%", "Predicted AN": "8.1%"}, "True Class AN": {"Predicted BN": "10%", "Predicted SN": "14%", "Predicted HN": "15%", "Predicted AN": "61%"}}}, "2D_CNN_performance (Mel spectrogram)": {"average_recognition_rate": "75.3%", "false_positive_rate": "3.52%", "confusion_matrix_percentages": {"True Class BN": {"Predicted BN": "96%", "Predicted SN": "0.92%", "Predicted HN": "1.2%", "Predicted AN": "1.4%"}, "True Class SN": {"Predicted BN": "10%", "Predicted SN": "73%", "Predicted HN": "4.5%", "Predicted AN": "12%"}, "True Class HN": {"Predicted BN": "8.6%", "Predicted SN": "4%", "Predicted HN": "76%", "Predicted AN": "11%"}, "True Class AN": {"Predicted BN": "7.6%", "Predicted SN": "10%", "Predicted HN": "5.6%", "Predicted AN": "77%"}}}, "Multi_Headed_2D_CNN_performance (Mel spectrogram + MFCCs)": {"average_recognition_rate": "79.67% (4.37% improvement over best 2D CNN)", "grouped_shouting_event_detection_rate": "94%", "confusion_matrix_percentages": {"True Class BN": {"Predicted BN": "96%", "Predicted SN": "1.3%", "Predicted HN": "1.4%", "Predicted AN": "1.1%"}, "True Class SN": {"Predicted BN": "7.7%", "Predicted SN": "79%", "Predicted HN": "3.1%", "Predicted AN": "10%"}, "True Class HN": {"Predicted BN": "5%", "Predicted SN": "3.1%", "Predicted HN": "84%", "Predicted AN": "6.7%"}, "True Class AN": {"Predicted BN": "5.5%", "Predicted SN": "9.7%", "Predicted HN": "8.3%", "Predicted AN": "76%"}}}}}, "temporal_info": {"publication_date": "23 October 2020", "submission_date": "19 August 2020", "revised_date": "24 September 2020", "accepted_date": "25 September 2020", "audio_window_size": "450 ms", "real_time_processing_comparison_times": ["~1000 ms in [6]", "104 to 1360 ms in [22]"]}, "geographical_data": {"database_collection_location": "Urban areas in Colombia, South America", "subject_location": "Universidad de Los Andes, Bogotá D.C. 111711, Colombia"}, "references": [{"number": 1, "description": "Vidal, J.B.I.; Kirchmaier, T. The Effect of Police Response Time on Crime Detection; Cep Discussion Papers; Centre for Economic Performance, LSE: London, UK, 2015."}, {"number": 2, "description": "Mabrouk, A.B.; Zagrouba, E. Abnormal behavior recognition for intelligent video surveillance systems: A review. Expert Syst. Appl. 2018, 91, 480–491."}, {"number": 3, "description": "Eng, H.L.; Toh, K.A.; Yau, W.Y.; Wang, J. DEWS: A live visual surveillance system for early drowning detection at pool. IEEE Trans. Circuits Syst. Video Technol. 2008, 18, 196–210."}, {"number": 4, "description": "Mubashir, M.; Shao, L.; Seed, L. A survey on fall detection: Principles and approaches. Neurocomputing 2013, 100, 144–152."}, {"number": 5, "description": "Burkhardt, F.; Paeschke, A.; Rolfes, M.; Sendlmeier, W.F.; Weiss, B. A database of German emotional speech. In Proceedings of the Ninth European Conference on Speech Communication and Technology, Lisbon, Portugal, 4–8 September 2005; pp. 1517–1520."}, {"number": 6, "description": "Huang, W.; Chiew, T.K.; Li, H.; Kok, T.S.; Biswas, J. Scream detection for home applications. In Proceedings of the 2010 5th IEEE Conference on Industrial Electronics and Applications, Taichung, Taiwan, 15–17 June 2010; pp. 2115–2120."}, {"number": 7, "description": "Parsons, C.E.; Young, K.S.; Craske, M.G.; Stein, A.L.; Kringelbach, M.L. Introducing the Oxford Vocal (OxVoc) Sounds database: A validated set of non-acted affective sounds from human infants, adults, and domestic animals. Front. Psychol. 2014, 5, 562."}, {"number": 8, "description": "Foggia, P.; Petkov, N.; Saggese, A.; Strisciuglio, N.; Vento, M. Reliable detection of audio events in highly noisy environments. Pattern Recognit. Lett. 2015, 65, 22–28."}, {"number": 9, "description": "Poria, S.; Cambria, E.; Howard, N.; Huang, G.B.; Hussain, A. Fusing audio, visual and textual clues for sentiment analysis from multimodal content. Neurocomputing 2016, 174, 50–59."}, {"number": 10, "description": "Strisciuglio, N.; Vento, M.; Petkov, N. Learning representations of sound using trainable COPE feature extractors. Pattern Recognit. 2019, 92, 25–36."}, {"number": 11, "description": "Dhanalakshmi, P.; Palanivel, S.; Ramalingam, V. Pattern classification models for classifying and indexing audio signals. Eng. Appl. Artif. Intell. 2011, 24, 350–357."}, {"number": 12, "description": "Zhao, J.; Mao, X.; Chen, L. Speech emotion recognition using deep 1D & 2D CNN LSTM networks. Biomed. Signal Process. Control 2019, 47, 424."}, {"number": 13, "description": "Alarcón-Paredes, A.; Francisco-García, V.; Guzmán-Guzmán, I.P.; Cantillo-Negrete, J.; Cuevas-Valencia, R.E.; Alonso-Silverio, G.A. An IoT-Based Non-Invasive Glucose Level Monitoring System Using Raspberry Pi. Appl. Sci. 2019, 9, 3046."}, {"number": 14, "description": "Ou, S.; Park, H.; Lee, J. Implementation of an obstacle recognition system for the blind. Appl. Sci. 2020, 10, 282."}, {"number": 15, "description": "Blue Microphones Snowball USB Microphone User Guide. 2009. Available online: https://s3.amazonaws.com/cd.bluemic.com/pdf/snowball/manual.pdf (accessed on 1 June 2020)."}, {"number": 16, "description": "Chou, W.; Juang, B.H. Pattern Recognition in Speech and Language Processing; CRC Press: Boca Raton, FL, USA, 2003."}, {"number": 17, "description": "Piczak, K.J. The details that matter: Frequency resolution of spectrograms in acoustic scene classification. In Detection and Classification of Acoustic Scenes and Events; Warsaw University of Technology: Munich, Germany, 2017; pp. 103–107."}, {"number": 18, "description": "Kadiri, S.R.; Alku, P. Mel-Frequency Cepstral Coefficients of Voice Source Waveforms for Classification of Phonation Types in Speech. In Interspeech; Department of Signal Processing and Acoustics, Aalto University: Espoo, Finland, 2019; pp. 2508–2512."}, {"number": 19, "description": "Umesh, S.; Cohen, L.; Nelson, D. Frequency warping and the Mel scale. IEEE Signal Process. Lett. 2002, 9, 104–107."}, {"number": 20, "description": "Lecun, Y.; Bengio, Y.; Hinton, G. Deep Learning; MIT Press: Cambridge, UK, 2015."}, {"number": 21, "description": "Velasco-Montero, D.; Fernández-Berni, J.; Carmona-Galán, R.; Rodríguez-Vázquez, Á. Performance analysis of real-time DNN inference on Raspberry Pi. In Proceedings of the Real-Time Image and Video Processing 2018. International Society for Optics and Photonics, Taichung, Taiwan, 9–12 December 2018; Volume 10670, p. 106700F."}, {"number": 22, "description": "Arslan, Y. A New Approach to Real Time Impulsive Sound Detection for Surveillance Applications. arXiv 2019, arXiv:1906.06586."}, {"number": 23, "description": "López, J.M.; Alonso, J.; Asensio, C.; Pavón, I.; Gascó, L.; de Arcas, G. A Digital Signal Processor Based Acoustic Sensor for Outdoor Noise Monitoring in Smart Cities. Sensors 2020, 20, 605."}]}