{"extracted_information": "This document provides a detailed comparison and technical analysis of real-time versus turn-based AI voice agent architectures. It covers the core architectural patterns, key performance metrics, available models and platforms (both proprietary and open-source), cost and scalability considerations, technical implementation challenges, testing methodologies, use case suitability, and customization options.", "specifications": {"audio_formats_telephony": "8 kHz, 8-bit µ-law (G.711)", "audio_formats_general": "16 kHz or 16 kHz+ (for most high-quality ASR), PCM raw audio, Opus, WAV, FLAC", "streaming_mechanisms": "WebRTC, WebSockets, streaming HTTP/gRPC", "telephony_protocols": "SIP/VoIP", "dtmf_handling": "Typically detected out-of-band by telephony provider via webhook events", "web_streaming_libraries": "LiveKit, mediasoup, Twilio, FastRTC (Python)", "asr_models_mentioned": "Whisper (OpenAI), DeepSpeech, Vosk, NeMo, Meta's MMS", "tts_models_mentioned": "VITS, FastSpeech, Google Cloud TTS, Amazon Polly, Microsoft's Custom Neural Voice, ElevenLabs, Resemble AI", "end_to_end_models_mentioned": "Moshi (Kyutai Labs), OpenAI's GPT-4o Audio, Google’s Gemini 2.5 Flash, Qwen-Audio (Alibaba), Ultravox (Fixie.ai), Meta’s MMS", "streaming_api_protocols": "OpenAI Voice API uses WebSockets, Google API uses gRPC streaming over HTTP/2"}, "pricing": {"usage_based_cloud_apis": [{"provider": "OpenAI GPT-4o (Realtime Preview)", "input_cost": "~$0.004/min", "output_cost": "~$0.008/min"}, {"provider": "Google Gemini Live", "cost_range": "~$0.0015–0.006/min"}], "compute_costs_self_hosting": {"description": "Run open-source models like Ultravox/Moshi on own infrastructure", "example_infra": "Ultravox 70B may need A100/H100 GPU per concurrent session", "gpu_cloud_costs": "~$2–$3/hr"}, "bandwidth_overhead": {"description": "Cost of streaming audio data over network", "per_stream_rate": "~8–64 kbps", "notes": "Minor cost per stream, adds up at scale; affected by codecs (G.711 vs G.729)"}, "enterprise_overhead": {"description": "Costs for SLAs, premium support, custom deployments, fallback systems", "notes": "Adds reliability/control, increases total cost of ownership (TCO)"}, "scalability_rate_limits": [{"provider": "OpenAI GPT-4o preview (Azure OpenAI, realtime mode)", "token_limit": "~800K tokens/min", "request_limit": "~1,000 req/min"}, {"provider": "Enterprise", "token_limit": "up to 30M tokens/min"}], "cost_strategy_notes": "Cloud APIs suitable for early-stage/low-volume (simple setup, scales with usage, higher cumulative cost at large volumes). Self-hosting for growing usage (lower marginal cost at scale, requires infra/DevOps). Enterprise for scale (focus on reliability, rate limits, support, flexibility). TCO includes processing, bandwidth, DevOps, redundancy, support."}, "features": [{"name": "Real-time Voice Agent", "description": "Processes spoken input and generates spoken output with minimal delay, often concurrently. Can start understanding and formulating a reply while the user is still talking.", "key_aspects": ["Streaming pipelines (audio processed in small chunks)", "AI may begin streaming response before user utterance finished", "Reduced awkward silence", "More natural and interactive feel"], "architecture": "Fully integrated streaming: Voice → LLM → Voice", "stt_processing": "Sends partial transcripts in real time", "llm_behavior": "Begins processing from partial input as user is speaking", "tts_synthesis": "Starts speaking as soon as first tokens generated, in a stream", "flexibility": "Less flexible (components must support real-time streaming/coordination)", "risks_challenges": "Requires stream orchestration to avoid mishearing or interrupting users", "user_experience": "Feels more 'alive', can begin replying before user finishes, can express emotions", "best_use_cases": "AI concierges, live support agents, multilingual assistants for fast-paced environments", "technical_requirements": "Requires low-latency infrastructure and session management for continuous real-time processing"}, {"name": "Traditional Voice Agent", "description": "Follows a turn-based pattern: user speaks, system listens silently, then responds after a pause.", "key_aspects": ["Waits for user to stop speaking", "Sends entire audio for transcription and response generation"], "architecture": "Voice → STT → LLM → TTS → Voice (sequential pipeline)", "stt_processing": "Waits for full sentence to transcribe", "llm_behavior": "Starts only after full STT output", "tts_synthesis": "Starts synthesizing audio only after entire text generated", "flexibility": "High (easy to swap out STT, TTS, LLM independently)", "risks_challenges": "Requires careful orchestration to minimize latency", "user_experience": "Structured and clear, but less dynamic", "best_use_cases": "Complex interactions requiring high accuracy (IVR, technical support)", "technical_requirements": "Lower (no need for streaming or session orchestration)"}, {"name": "Integrated Speech-to-Speech Architecture (Voice → LLM → Voice)", "description": "Single model takes audio input and generates audio output directly without separate STT/TTS modules.", "examples": "Moshi (Kyutai Labs), OpenAI’s GPT-4o Audio, Google’s Gemini 2.5 Flash voice capabilities", "benefits": "Minimize latency, preserve conversational flow, potential handling of paralinguistic cues", "challenges": "Cutting-edge and complex, may lack flexibility/accuracy of separate components (in current iterations)"}, {"name": "Hybrid Architecture (Voice -> (STT) -> LLM -> (TTS) -> Voice)", "description": "Balances integrated models with modular components. Input/Output can be processed directly by multimodal LLM or pass through separate STT/TTS.", "key_configurations": [{"config": "Voice → LLM → Voice (Direct)", "input": "Voice", "output": "Voice", "example_models": "Moshi, Qwen-Audio"}, {"config": "Voice → LLM → Text → TTS (Partial)", "input": "Voice", "output": "Voice", "example_models": "Ultravox (planned)"}, {"config": "Voice → STT → LLM → Voice (Modular)", "input": "Text", "output": "Voice", "example_models": "Meta’s MMS, OpenAI GPT-4o + codec voice"}, {"config": "Voice → STT → LLM → TTS → Voice (Traditional)", "input": "Text", "output": "Voice", "example_models": "Google STT + GPT-4o + Azure TTS"}], "benefits": "Versatility, flexibility (choose components), balance between integrated and modular.", "challenges": "TTS stage can add latency/complexity (for Voice → STT → LLM → TTS → Voice), Custom voice control limited if LLM handles voice generation directly, STT-to-LLM orchestration needed."}, {"name": "Voice Customization", "description": "Tailoring the agent's voice output.", "methods": ["Voice library selection (for separate TTS)", "Voice Cloning (create custom voice from target recordings - e.g., ElevenLabs, Microsoft Custom Neural Voice, Resemble AI)", "Multilingual/Accent customization", "Swapping vocoder voice profile (with unit vocoder output like Ultravox plans)"]}, {"name": "Language Model Fine-Tuning", "description": "Adapting the LLM's knowledge and behavior.", "types": ["Domain fine-tuning (improve understanding/accuracy on specific topics, e.g., medical, product details)", "Behavior fine-tuning (enforce specific conversation style/phrasing)", "Fine-tuning with open-source LLMs (e.g., LoRA with Llama 3 and Ultravox pipeline)"], "benefits": "Higher quality, shorter prompts (saves latency/cost)", "requirements": "Dataset preparation (representative data, synthetic or real), data privacy consideration."}, {"name": "Prompt Engineering and Dynamic Prompts", "description": "Guiding the model's behavior and content through prompts.", "methods": ["Crafting detailed system prompts (persona, knowledge, goals)", "Using role-playing and example dialogues in prompts", "Injecting variables and updated instructions dynamically (runtime customization)", "Maintaining a rolling prompt for conversation state (handle context window limits)"]}, {"name": "Integrating External Knowledge and Tools", "description": "Giving the agent access to external information and capabilities.", "methods": ["Tool integration / Function Calling (agent calls APIs for latest info, e.g., check inventory)", "Retrieval Augmentation (agent retrieves relevant text from knowledge base/vector database and includes in prompt for factual accuracy)"]}, {"name": "Acoustic Echo Cancellation (AEC)", "description": "Useful WebRTC feature to handle echo, especially on speakerphone.", "notes": "Phone networks often handle this, but may be needed in custom pipelines."}, {"name": "Noise Suppression", "description": "Algorithm applied to input audio before ASR to remove background noise (e.g., RNNoise, Picovoice’s Koala).", "tradeoff": "Can slightly distort voice or consume CPU."}, {"name": "Voice Activity Detection (VAD)", "description": "Detects when speech is present. Used for turn-taking. Can be tuned for sensitivity in noisy conditions.", "robust_approach": "Combine with ASR confidence; treat as end-of-utterance if silence + ASR final for duration (e.g., 500ms)."}, {"name": "Barge-in", "description": "Ability for the user to interrupt the agent's speech. Requires monitoring the mic while TTS is active.", "standard_in": "Full-duplex setups."}], "statistics": {"proprietary_platforms_performance": [{"model": "OpenAI GPT-4o (Realtime Preview)", "provider": "OpenAI / Azure", "ttft": "~280 ms (~0.25-0.3 s)", "token_generation_speed": "~70–100 tokens/second"}, {"model": "Google Gemini 2.0 Flash", "provider": "Google / DeepMind", "ttft": "~280 ms (~0.28 s)", "token_generation_speed": "~155–160 tokens/second"}], "open_source_platforms_performance": [{"model": "Ultravox (smaller variant)", "provider": "Fixie.ai", "ttft": "~190 ms", "token_generation_speed": "~200+ tokens/sec"}, {"model": "Moshi", "provider": "Kyutai Labs", "ttft": "~160 ms", "token_generation_speed": "Not token-based (generates speech waveform directly)"}], "word_error_rate_examples": [{"source": "Meta AI research on streaming LLM-based ASR", "dataset": "Librispeech test-clean", "wer": "about 3.0%"}, {"source": "Meta AI research on streaming LLM-based ASR", "dataset": "Librispeech test-other", "wer": "~7.4%"}], "real_time_factor_examples": [{"component": "STT engine", "rtf": "e.g., 0.2x real time (very fast)"}, {"component": "LLM token generation", "rtf_proxy": "e.g., 50 tokens/sec"}, {"component": "TTS synthesis", "rtf": "e.g., 0.1 or better (10 seconds of speech in 1 second processing)"}], "human_response_latency": "Often around 200 ms (gap between speakers)"}, "temporal_info": {"release_dates_mentions": [{"model": "AI voice agents shifting to real-time", "notes": "Recent breakthroughs"}, {"model": "Meta's MMS", "date": "May 2023"}, {"model": "AI Voice Agents: Real-Time vs Turn-Based (TTS/STT) Architecture", "publication_date": "09 May 2025"}]}, "geographical_data": {"notes": "Data center location relative to telephony ingress affects latency. Softcery OÜ registered in Estonia, Tallinn."}, "references": ["https://www.isca-archive.org/interspeech_2024/seide24_interspeech.pdf", "https://github.com/kyutai-labs/moshi", "https://platform.openai.com/docs/models/gpt-4o-audio-preview", "https://deepmind.google/technologies/gemini/flash/", "https://www.ultravox.ai/", "https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/", "https://www.alibabacloud.com/blog/alibaba-cloud-launches-qwen2-audio-model-to-analyze-speech-and-audio_601584", "https://softcery.com/lab/how-to-choose-stt-tts-for-ai-voice-agents-in-2025-a-comprehensive-guide/", "https://news.ycombinator.com/item?id=41743327", "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/", "https://ai.google.dev/gemini-api/docs/models", "https://rumn.medium.com/benchmarking-llm-performance-token-per-second-tps-time-to-first-token-ttft-and-gpu-usage-8c50ee8387fa", "https://www.rev.com/resources/what-is-wer-what-does-word-error-rate-mean", "https://www.openslr.org/12", "https://www.pipecat.ai/", "http://daily.co/", "https://www.facebook.com/livebox4.0/", "https://webrtc.org/", "https://fastrtc.org/", "https://alphacephei.com/vosk/", "https://www.nemo-ai.com/en/", "https://docs.coqui.ai/en/latest/models/vits.html", "https://arxiv.org/abs/1905.09263", "https://www.mathworks.com/help/audio/ug/acoustic-echo-cancellation-aec.html", "https://livekit.io/", "https://www.twilio.com/", "https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API", "https://grpc.io/", "https://en.wikipedia.org/wiki/WAV", "https://en.wikipedia.org/wiki/FLAC", "https://www.google.com/aclk?sa=l&ai=DChcSEwiujJmNsOGMAxXFimgJHQQwKB4YABAAGgJ3Zg&co=1&gclid=CjwKCAjw8IfABhBXEiwAxRHlsEkBiLRC_uxDJAwzXSy8je7B1peof15bd0kDrnPysL3uZ4iZTsWpCxoCPTIQAvD_BwE&sig=AOD64_1LYf5aSf5Px_PlNRcCU2-yV58wFQ&q=&adurl=&ved=2ahUKEwjH75KNsOGMAxVlcfEDHcT1IQkQ0Qx6BAgTEAE (Whisper)", "https://learn.microsoft.com/en-us/connectors/nexmo/#:~:text=Nexmo%2C%20the%20Vonage%20API%20Platform,%3A%2F%2Fwww.nexmo.com%2F", "https://en.wikipedia.org/wiki/DTMF_signaling", "https://github.com/xiph/rnnoise", "https://picovoice.ai/platform/koala/", "https://en.wikipedia.org/wiki/WebSocket#:~:text=WebSocket%20is%20a%20computer%20communications,as%20RFC%206455%20in%202011.", "https://www.google.com/aclk?sa=l&ai=DChcSEwi9i_-mteGMAxX7UZEFHWAQEZoYABAAGgJscg&co=1&gclid=CjwKCAjw8IfABhBXEiwAxRHlsHShpuAJjFdDBRYjZD4jUy7JlTwyv23ikZ5FEJHfLtXP-uPoLc0teBoCD1kQAvD_BwE&sig=AOD64_0mbZCMKC_sMM8eqxH2M47eXMWF2A&q=&adurl=&ved=2ahUKEwji1_SmteGMAxWvFhAIHYAdFpQQ0Qx6BAgJEAE (ElevenLabs)", "https://learn.microsoft.com/en-us/azure/ai-services/speech-service/custom-neural-voice", "https://www.resemble.ai/", "https://www.youtube.com/watch?v=K1TbRsgwWd4", "https://github.com/fixie-ai/ultravox", "https://platform.openai.com/docs/models/gpt-3-5", "https://openai.com/index/gpt-4/", "https://arxiv.org/abs/2106.09685 (LoRA)", "https://www.llama.com/"]}