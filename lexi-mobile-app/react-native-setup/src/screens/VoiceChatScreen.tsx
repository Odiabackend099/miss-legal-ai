/**\n * VoiceChatScreen - Main voice interaction interface\n * The heart of Lexi Voice AI with Nigerian cultural optimization\n */\n\nimport React, { useState, useEffect, useRef, useCallback } from 'react';\nimport {\n  StyleSheet,\n  Text,\n  View,\n  TouchableOpacity,\n  Alert,\n  Dimensions,\n  Animated,\n  ScrollView,\n  ActivityIndicator,\n  Platform,\n} from 'react-native';\nimport Icon from 'react-native-vector-icons/MaterialCommunityIcons';\nimport LinearGradient from 'react-native-linear-gradient';\nimport { useFocusEffect } from '@react-navigation/native';\n\n// Services\nimport { VoiceService, VoiceProcessingResult, SupportedLanguage } from '../services/VoiceService';\nimport { SupabaseService } from '../services/SupabaseService';\n\n// Components\nimport VoiceWaveform from '../components/VoiceWaveform';\nimport LanguageSelector from '../components/LanguageSelector';\nimport ChatMessage from '../components/ChatMessage';\nimport NetworkStatus from '../components/NetworkStatus';\n\n// Types\nimport { VoiceMessage, ConversationHistory } from '../types/user';\n\n// Constants\nconst { width: SCREEN_WIDTH, height: SCREEN_HEIGHT } = Dimensions.get('window');\nconst COLORS = {\n  primary: '#008751', // Nigerian Green\n  secondary: '#FFFFFF',\n  accent: '#228B22',\n  background: '#F5F5F5',\n  text: '#333333',\n  textLight: '#666666',\n  error: '#FF4444',\n  success: '#00AA44',\n  warning: '#FFAA00',\n};\n\n// Nigerian greetings by language\nconst GREETINGS = {\n  english: \"Hello! I'm Lexi, your Nigerian AI assistant. How can I help you today?\",\n  pidgin: \"How far! I be Lexi, your AI assistant. Wetin I fit do for you today?\",\n  hausa: \"Sannu! Ni Lexi ne, mai taimaka AI. Me zan iya taimaka maka yau?\",\n  igbo: \"Ndewo! Ab·ª• m Lexi, onye enyemaka AI g·ªã. Kedu ka m ga-esi nyere g·ªã aka taa?\",\n  yoruba: \"·∫∏ k√∫ √†√°r·ªçÃÄ! √àmi ni Lexi, ol√πr√†nl·ªçÃÅw·ªçÃÅ AI y√≠n. B√°wo ni mo ·π£e l√® r√†n y√≠n l·ªçÃÅw·ªçÃÅ l√≥n√¨√≠?\"\n};\n\ninterface VoiceChatScreenProps {\n  navigation: any;\n}\n\nconst VoiceChatScreen: React.FC<VoiceChatScreenProps> = ({ navigation }) => {\n  // State management\n  const [isRecording, setIsRecording] = useState(false);\n  const [isProcessing, setIsProcessing] = useState(false);\n  const [isPlaying, setIsPlaying] = useState(false);\n  const [currentLanguage, setCurrentLanguage] = useState<SupportedLanguage>('english');\n  const [messages, setMessages] = useState<VoiceMessage[]>([]);\n  const [recordingDuration, setRecordingDuration] = useState(0);\n  const [isConnected, setIsConnected] = useState(true);\n  const [networkQuality, setNetworkQuality] = useState<'excellent' | 'good' | 'poor'>('good');\n  \n  // Animation refs\n  const pulseAnim = useRef(new Animated.Value(1)).current;\n  const waveformAnim = useRef(new Animated.Value(0)).current;\n  const buttonScaleAnim = useRef(new Animated.Value(1)).current;\n  \n  // Timers\n  const recordingTimer = useRef<NodeJS.Timeout | null>(null);\n  const scrollViewRef = useRef<ScrollView>(null);\n  \n  // Effects\n  useEffect(() => {\n    initializeScreen();\n    return cleanup;\n  }, []);\n  \n  useFocusEffect(\n    useCallback(() => {\n      // Screen focused - refresh state\n      checkNetworkStatus();\n      loadConversationHistory();\n    }, [])\n  );\n  \n  // Initialize screen\n  const initializeScreen = async () => {\n    try {\n      console.log('üé§ Initializing Voice Chat Screen...');\n      \n      // Get user's preferred language\n      const language = VoiceService.getCurrentLanguage();\n      setCurrentLanguage(language);\n      \n      // Add welcome message\n      addWelcomeMessage(language);\n      \n      // Check network status\n      await checkNetworkStatus();\n      \n      console.log('‚úÖ Voice Chat Screen initialized');\n      \n    } catch (error) {\n      console.error('‚ùå Failed to initialize Voice Chat Screen:', error);\n      Alert.alert(\n        'Initialization Error',\n        'Failed to initialize voice chat. Please try again.',\n        [{ text: 'OK' }]\n      );\n    }\n  };\n  \n  // Add welcome message\n  const addWelcomeMessage = (language: SupportedLanguage) => {\n    const welcomeMessage: VoiceMessage = {\n      id: 'welcome_' + Date.now(),\n      type: 'ai_response',\n      content: GREETINGS[language],\n      timestamp: new Date().toISOString(),\n      language: language,\n      audioPath: null,\n      processingTime: 0,\n    };\n    \n    setMessages([welcomeMessage]);\n  };\n  \n  // Check network status\n  const checkNetworkStatus = async () => {\n    try {\n      // In a real implementation, use @react-native-community/netinfo\n      setIsConnected(true);\n      setNetworkQuality('good');\n    } catch (error) {\n      console.error('‚ùå Network check failed:', error);\n      setIsConnected(false);\n    }\n  };\n  \n  // Load conversation history\n  const loadConversationHistory = async () => {\n    try {\n      const conversations = await SupabaseService.getConversationHistory(1);\n      if (conversations.length > 0) {\n        const lastConversation = conversations[0];\n        if (lastConversation.messages.length > 1) {\n          // Load last few messages for context\n          const recentMessages = lastConversation.messages.slice(-5);\n          setMessages(prev => [...prev, ...recentMessages]);\n        }\n      }\n    } catch (error) {\n      console.error('‚ùå Failed to load conversation history:', error);\n    }\n  };\n  \n  // Start voice recording\n  const startRecording = async () => {\n    try {\n      if (!isConnected && networkQuality === 'poor') {\n        Alert.alert(\n          'Poor Connection',\n          'Your network connection is poor. Voice recognition may be affected.',\n          [\n            { text: 'Cancel', style: 'cancel' },\n            { text: 'Continue', onPress: () => performStartRecording() }\n          ]\n        );\n        return;\n      }\n      \n      await performStartRecording();\n      \n    } catch (error) {\n      console.error('‚ùå Failed to start recording:', error);\n      Alert.alert(\n        'Recording Error',\n        'Failed to start voice recording. Please check your microphone permissions.',\n        [{ text: 'OK' }]\n      );\n    }\n  };\n  \n  const performStartRecording = async () => {\n    console.log('üé§ Starting voice recording...');\n    \n    // Start recording\n    await VoiceService.startRecording();\n    setIsRecording(true);\n    setRecordingDuration(0);\n    \n    // Start animations\n    startRecordingAnimations();\n    \n    // Start timer\n    recordingTimer.current = setInterval(() => {\n      setRecordingDuration(prev => prev + 1);\n    }, 1000);\n    \n    // Add user message placeholder\n    const userMessage: VoiceMessage = {\n      id: 'user_' + Date.now(),\n      type: 'user_voice',\n      content: 'Recording...', // Will be updated after processing\n      timestamp: new Date().toISOString(),\n      language: currentLanguage,\n      audioPath: null,\n      processingTime: 0,\n    };\n    \n    setMessages(prev => [...prev, userMessage]);\n    scrollToBottom();\n  };\n  \n  // Stop voice recording\n  const stopRecording = async () => {\n    try {\n      if (!isRecording) return;\n      \n      console.log('üõë Stopping voice recording...');\n      \n      // Stop recording\n      const audioPath = await VoiceService.stopRecording();\n      setIsRecording(false);\n      \n      // Stop animations and timer\n      stopRecordingAnimations();\n      if (recordingTimer.current) {\n        clearInterval(recordingTimer.current);\n        recordingTimer.current = null;\n      }\n      \n      // Process the recording\n      await processVoiceRecording(audioPath);\n      \n    } catch (error) {\n      console.error('‚ùå Failed to stop recording:', error);\n      setIsRecording(false);\n      stopRecordingAnimations();\n      \n      Alert.alert(\n        'Recording Error',\n        'Failed to process voice recording. Please try again.',\n        [{ text: 'OK' }]\n      );\n    }\n  };\n  \n  // Process voice recording\n  const processVoiceRecording = async (audioPath: string) => {\n    try {\n      setIsProcessing(true);\n      \n      console.log('üß† Processing voice recording...');\n      \n      // Process with AI\n      const result: VoiceProcessingResult = await VoiceService.processVoiceRecording(audioPath);\n      \n      if (result.success) {\n        // Update user message with transcription\n        setMessages(prev => {\n          const updated = [...prev];\n          const lastUserMessage = updated[updated.length - 1];\n          if (lastUserMessage.type === 'user_voice') {\n            lastUserMessage.content = result.transcription || 'Voice message';\n            lastUserMessage.audioPath = audioPath;\n            lastUserMessage.processingTime = result.processingTime || 0;\n          }\n          return updated;\n        });\n        \n        // Add AI response\n        if (result.response) {\n          const aiMessage: VoiceMessage = {\n            id: 'ai_' + Date.now(),\n            type: 'ai_response',\n            content: result.response,\n            timestamp: new Date().toISOString(),\n            language: currentLanguage,\n            audioPath: result.responseAudioPath || null,\n            processingTime: result.processingTime || 0,\n          };\n          \n          setMessages(prev => [...prev, aiMessage]);\n          \n          // Play AI response if available\n          if (result.responseAudioPath) {\n            await playAIResponse(result.responseAudioPath);\n          }\n        }\n        \n        // Save conversation\n        await saveConversation();\n        \n      } else {\n        throw new Error(result.error || 'Processing failed');\n      }\n      \n    } catch (error) {\n      console.error('‚ùå Voice processing failed:', error);\n      \n      // Add error message\n      const errorMessage: VoiceMessage = {\n        id: 'error_' + Date.now(),\n        type: 'ai_response',\n        content: getErrorMessage(currentLanguage),\n        timestamp: new Date().toISOString(),\n        language: currentLanguage,\n        audioPath: null,\n        processingTime: 0,\n      };\n      \n      setMessages(prev => [...prev, errorMessage]);\n      \n    } finally {\n      setIsProcessing(false);\n      scrollToBottom();\n    }\n  };\n  \n  // Play AI response\n  const playAIResponse = async (audioPath: string) => {\n    try {\n      setIsPlaying(true);\n      await VoiceService.playAudioResponse(audioPath);\n    } catch (error) {\n      console.error('‚ùå Failed to play AI response:', error);\n    } finally {\n      setIsPlaying(false);\n    }\n  };\n  \n  // Save conversation\n  const saveConversation = async () => {\n    try {\n      if (messages.length < 2) return; // Need at least user message and AI response\n      \n      const conversation: Omit<ConversationHistory, 'id' | 'userId' | 'createdAt'> = {\n        title: generateConversationTitle(),\n        messages: messages,\n        language: currentLanguage,\n        duration: calculateConversationDuration(),\n        messageCount: messages.length,\n      };\n      \n      await SupabaseService.saveConversation(conversation);\n      \n    } catch (error) {\n      console.error('‚ùå Failed to save conversation:', error);\n    }\n  };\n  \n  // Generate conversation title\n  const generateConversationTitle = (): string => {\n    const now = new Date();\n    const timeString = now.toLocaleTimeString('en-NG', { \n      hour: '2-digit', \n      minute: '2-digit' \n    });\n    \n    const titles = {\n      english: `Chat at ${timeString}`,\n      pidgin: `Gist for ${timeString}`,\n      hausa: `Hira a ${timeString}`,\n      igbo: `Nk·ªçwa na ${timeString}`,\n      yoruba: `√åj√≠r√≤r√≤ n√≠ ${timeString}`\n    };\n    \n    return titles[currentLanguage] || titles.english;\n  };\n  \n  // Calculate conversation duration\n  const calculateConversationDuration = (): number => {\n    if (messages.length < 2) return 0;\n    \n    const firstMessage = messages[0];\n    const lastMessage = messages[messages.length - 1];\n    \n    const start = new Date(firstMessage.timestamp).getTime();\n    const end = new Date(lastMessage.timestamp).getTime();\n    \n    return Math.round((end - start) / 1000); // Duration in seconds\n  };\n  \n  // Language change handler\n  const handleLanguageChange = async (language: SupportedLanguage) => {\n    try {\n      setCurrentLanguage(language);\n      await VoiceService.setLanguage(language);\n      \n      // Add language change message\n      const changeMessage: VoiceMessage = {\n        id: 'lang_change_' + Date.now(),\n        type: 'ai_response',\n        content: GREETINGS[language],\n        timestamp: new Date().toISOString(),\n        language: language,\n        audioPath: null,\n        processingTime: 0,\n      };\n      \n      setMessages(prev => [...prev, changeMessage]);\n      scrollToBottom();\n      \n    } catch (error) {\n      console.error('‚ùå Failed to change language:', error);\n    }\n  };\n  \n  // Animation functions\n  const startRecordingAnimations = () => {\n    // Pulse animation\n    Animated.loop(\n      Animated.sequence([\n        Animated.timing(pulseAnim, {\n          toValue: 1.2,\n          duration: 1000,\n          useNativeDriver: true,\n        }),\n        Animated.timing(pulseAnim, {\n          toValue: 1,\n          duration: 1000,\n          useNativeDriver: true,\n        }),\n      ])\n    ).start();\n    \n    // Waveform animation\n    Animated.loop(\n      Animated.timing(waveformAnim, {\n        toValue: 1,\n        duration: 2000,\n        useNativeDriver: true,\n      })\n    ).start();\n  };\n  \n  const stopRecordingAnimations = () => {\n    pulseAnim.stopAnimation(() => pulseAnim.setValue(1));\n    waveformAnim.stopAnimation(() => waveformAnim.setValue(0));\n  };\n  \n  const animateButtonPress = () => {\n    Animated.sequence([\n      Animated.timing(buttonScaleAnim, {\n        toValue: 0.95,\n        duration: 100,\n        useNativeDriver: true,\n      }),\n      Animated.timing(buttonScaleAnim, {\n        toValue: 1,\n        duration: 100,\n        useNativeDriver: true,\n      }),\n    ]).start();\n  };\n  \n  // Utility functions\n  const scrollToBottom = () => {\n    setTimeout(() => {\n      scrollViewRef.current?.scrollToEnd({ animated: true });\n    }, 100);\n  };\n  \n  const formatDuration = (seconds: number): string => {\n    const mins = Math.floor(seconds / 60);\n    const secs = seconds % 60;\n    return `${mins}:${secs.toString().padStart(2, '0')}`;\n  };\n  \n  const getErrorMessage = (language: SupportedLanguage): string => {\n    const errors = {\n      english: \"Sorry, I couldn't process that. Please try again.\",\n      pidgin: \"Sorry o, I no fit understand that one. Try again.\",\n      hausa: \"Yi hakuri, ban iya fahimta ba. Ka sake gwadawa.\",\n      igbo: \"Ndo, ap·ª•gh·ªã m ·ªãgh·ªçta nke ah·ª•. Nwalee ·ªçz·ªç.\",\n      yoruba: \"P·∫πl·∫π o, mi √≤ l√® l√≥ye iy·∫πn. ·∫∏ gb√¨y√†nj√∫ l·∫πÃÅ·∫πÃÄkan si.\"\n    };\n    \n    return errors[language] || errors.english;\n  };\n  \n  // Cleanup\n  const cleanup = () => {\n    if (recordingTimer.current) {\n      clearInterval(recordingTimer.current);\n    }\n    stopRecordingAnimations();\n  };\n  \n  // Render functions\n  const renderHeader = () => (\n    <View style={styles.header}>\n      <LinearGradient\n        colors={[COLORS.primary, COLORS.accent]}\n        style={styles.headerGradient}\n      >\n        <View style={styles.headerContent}>\n          <View style={styles.headerLeft}>\n            <Icon name=\"robot\" size={24} color={COLORS.secondary} />\n            <Text style={styles.headerTitle}>Lexi Assistant</Text>\n          </View>\n          \n          <View style={styles.headerRight}>\n            <NetworkStatus \n              isConnected={isConnected}\n              quality={networkQuality}\n            />\n            \n            <LanguageSelector\n              currentLanguage={currentLanguage}\n              onLanguageChange={handleLanguageChange}\n            />\n          </View>\n        </View>\n      </LinearGradient>\n    </View>\n  );\n  \n  const renderMessages = () => (\n    <ScrollView\n      ref={scrollViewRef}\n      style={styles.messagesContainer}\n      contentContainerStyle={styles.messagesContent}\n      showsVerticalScrollIndicator={false}\n    >\n      {messages.map((message) => (\n        <ChatMessage\n          key={message.id}\n          message={message}\n          onPlayAudio={playAIResponse}\n          currentLanguage={currentLanguage}\n        />\n      ))}\n      \n      {isProcessing && (\n        <View style={styles.processingIndicator}>\n          <ActivityIndicator size=\"small\" color={COLORS.primary} />\n          <Text style={styles.processingText}>Processing...</Text>\n        </View>\n      )}\n    </ScrollView>\n  );\n  \n  const renderVoiceControls = () => (\n    <View style={styles.voiceControls}>\n      {isRecording && (\n        <View style={styles.recordingInfo}>\n          <VoiceWaveform \n            isActive={isRecording}\n            animation={waveformAnim}\n          />\n          <Text style={styles.recordingDuration}>\n            {formatDuration(recordingDuration)}\n          </Text>\n        </View>\n      )}\n      \n      <View style={styles.voiceButtonContainer}>\n        <Animated.View\n          style={[\n            styles.voiceButtonWrapper,\n            {\n              transform: [\n                { scale: buttonScaleAnim },\n                { scale: pulseAnim }\n              ]\n            }\n          ]}\n        >\n          <TouchableOpacity\n            style={[\n              styles.voiceButton,\n              isRecording && styles.voiceButtonRecording,\n              isProcessing && styles.voiceButtonProcessing\n            ]}\n            onPress={() => {\n              animateButtonPress();\n              isRecording ? stopRecording() : startRecording();\n            }}\n            disabled={isProcessing}\n            activeOpacity={0.8}\n          >\n            <LinearGradient\n              colors={isRecording \n                ? ['#FF4444', '#CC0000'] \n                : isProcessing \n                  ? ['#FFAA00', '#FF8800']\n                  : [COLORS.primary, COLORS.accent]\n              }\n              style={styles.voiceButtonGradient}\n            >\n              <Icon\n                name={isRecording ? 'stop' : isProcessing ? 'loading' : 'microphone'}\n                size={32}\n                color={COLORS.secondary}\n              />\n            </LinearGradient>\n          </TouchableOpacity>\n        </Animated.View>\n      </View>\n      \n      <Text style={styles.voiceHint}>\n        {isRecording \n          ? 'Tap to stop recording'\n          : isProcessing \n            ? 'Processing your voice...'\n            : 'Tap to start speaking'\n        }\n      </Text>\n    </View>\n  );\n  \n  // Main render\n  return (\n    <View style={styles.container}>\n      {renderHeader()}\n      {renderMessages()}\n      {renderVoiceControls()}\n    </View>\n  );\n};\n\n// Styles\nconst styles = StyleSheet.create({\n  container: {\n    flex: 1,\n    backgroundColor: COLORS.background,\n  },\n  \n  header: {\n    paddingTop: Platform.OS === 'ios' ? 44 : 0,\n  },\n  \n  headerGradient: {\n    paddingVertical: 16,\n    paddingHorizontal: 20,\n  },\n  \n  headerContent: {\n    flexDirection: 'row',\n    justifyContent: 'space-between',\n    alignItems: 'center',\n  },\n  \n  headerLeft: {\n    flexDirection: 'row',\n    alignItems: 'center',\n  },\n  \n  headerTitle: {\n    fontSize: 18,\n    fontWeight: 'bold',\n    color: COLORS.secondary,\n    marginLeft: 8,\n  },\n  \n  headerRight: {\n    flexDirection: 'row',\n    alignItems: 'center',\n  },\n  \n  messagesContainer: {\n    flex: 1,\n    paddingHorizontal: 16,\n  },\n  \n  messagesContent: {\n    paddingVertical: 16,\n  },\n  \n  processingIndicator: {\n    flexDirection: 'row',\n    alignItems: 'center',\n    justifyContent: 'center',\n    paddingVertical: 16,\n  },\n  \n  processingText: {\n    marginLeft: 8,\n    fontSize: 14,\n    color: COLORS.textLight,\n  },\n  \n  voiceControls: {\n    paddingHorizontal: 20,\n    paddingVertical: 24,\n    backgroundColor: COLORS.secondary,\n    borderTopWidth: 1,\n    borderTopColor: '#E0E0E0',\n  },\n  \n  recordingInfo: {\n    alignItems: 'center',\n    marginBottom: 16,\n  },\n  \n  recordingDuration: {\n    fontSize: 16,\n    fontWeight: 'bold',\n    color: COLORS.primary,\n    marginTop: 8,\n  },\n  \n  voiceButtonContainer: {\n    alignItems: 'center',\n    marginBottom: 16,\n  },\n  \n  voiceButtonWrapper: {\n    shadowColor: '#000',\n    shadowOffset: {\n      width: 0,\n      height: 4,\n    },\n    shadowOpacity: 0.3,\n    shadowRadius: 8,\n    elevation: 8,\n  },\n  \n  voiceButton: {\n    width: 80,\n    height: 80,\n    borderRadius: 40,\n    overflow: 'hidden',\n  },\n  \n  voiceButtonRecording: {\n    // Additional styles for recording state\n  },\n  \n  voiceButtonProcessing: {\n    // Additional styles for processing state\n  },\n  \n  voiceButtonGradient: {\n    flex: 1,\n    justifyContent: 'center',\n    alignItems: 'center',\n  },\n  \n  voiceHint: {\n    textAlign: 'center',\n    fontSize: 14,\n    color: COLORS.textLight,\n    fontStyle: 'italic',\n  },\n});\n\nexport default VoiceChatScreen;"
